<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>speclet.analysis.pymc3_analysis API documentation</title>
<meta name="description" content="Functions to aid in the analysis of PyMC3 models." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>speclet.analysis.pymc3_analysis</code></h1>
</header>
<section id="section-intro">
<p>Functions to aid in the analysis of PyMC3 models.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Functions to aid in the analysis of PyMC3 models.&#34;&#34;&#34;

import re
from typing import Optional, Union

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotnine as gg
import pymc3 as pm
import seaborn as sns
from matplotlib.axes import Axes
from matplotlib.figure import Figure

from speclet.plot.color_pal import SeabornColor


def plot_all_priors(
    prior_predictive: dict[str, np.ndarray],
    subplots: tuple[int, int],
    figsize: tuple[float, float],
    samples: int = 1000,
    rm_var_regex: str = &#34;log__|logodds_&#34;,
) -&gt; tuple[Figure, Axes]:
    &#34;&#34;&#34;Plot all priors of a PyMC3 model.

    Args:
        prior_predictive (dict[str, np.ndarray]): The results of sampling from the
          priors of a PyMC3 model.
        subplots (tuple[int, int]): How many subplots to create.
        figsize (tuple[float, float]): The size of the final figure.
        samples (int, optional): The number of samples from the distributions to use.
          This can help the performance of the plotting if there are many samples.
          Defaults to 1000.
        rm_var_regex (str, optional): A regular expression for variables to remove.
          Defaults to &#34;log__|logodds_&#34;.

    Returns:
        tuple[Figure, Axes]: The matplotlib figure and array
          of axes.
    &#34;&#34;&#34;
    model_vars: list[str] = []
    for x in prior_predictive:
        if not re.search(rm_var_regex, x):
            model_vars.append(x)

    model_vars.sort()
    model_vars.sort(key=lambda x: -len(x))

    fig, axes = plt.subplots(subplots[0], subplots[1], figsize=figsize)
    for var, ax in zip(model_vars, axes.flatten()):
        sns.kdeplot(x=np.random.choice(prior_predictive[var].flatten(), samples), ax=ax)
        ax.set_title(var)

    fig.tight_layout()
    return fig, axes


def _plot_vi_axes_limits(
    yvals: np.ndarray, x_start: Optional[Union[int, float]]
) -&gt; tuple[list[int], list[float]]:
    yvals = yvals.copy()[np.isfinite(yvals)]
    x_lims: list[int] = [0, len(yvals)]
    if x_start is None:
        pass
    elif isinstance(x_start, float) and x_start &lt;= 1.0:
        x_lims[0] = int(x_start * len(yvals))
    else:
        x_lims[0] = int(x_start)

    y_lims: list[float] = [min(yvals), max(yvals[x_lims[0] :])]

    return x_lims, y_lims


def _advi_hist_rolling_avg(df: pd.DataFrame, window: int) -&gt; pd.DataFrame:
    df[&#34;loss_rolling_avg&#34;] = (
        df[[&#34;loss&#34;]].rolling(window=window, center=True).mean()[&#34;loss&#34;]
    )
    return df


def plot_vi_hist(
    approx: pm.Approximation,
    y_log: bool = False,
    x_start: Optional[Union[int, float]] = None,
    rolling_window: int = 100,
) -&gt; gg.ggplot:
    &#34;&#34;&#34;Plot the history of fitting using Variational Inference.

    Args:
        approx (pm.variational.Approximation): The approximation attribute from the
          VI object.

    Returns:
        gg.ggplot: A plot showing the fitting history.
    &#34;&#34;&#34;
    y = &#34;np.log(loss)&#34; if y_log else &#34;loss&#34;
    rolling_y = &#34;np.log(loss_rolling_avg)&#34; if y_log else &#34;loss_rolling_avg&#34;
    d = (
        pd.DataFrame({&#34;loss&#34;: approx.hist})
        .assign(step=lambda d: np.arange(d.shape[0]))
        .pipe(_advi_hist_rolling_avg, window=rolling_window)
    )

    _x_lims, _y_lims = _plot_vi_axes_limits(approx.hist, x_start)
    if y_log:
        _y_lims = np.log(_y_lims).tolist()

    return (
        gg.ggplot(d, gg.aes(x=&#34;step&#34;))
        + gg.geom_line(gg.aes(y=y), size=0.5, alpha=0.75, color=&#34;black&#34;)
        + gg.geom_line(gg.aes(y=rolling_y), size=0.5, alpha=0.9, color=SeabornColor.RED)
        + gg.scale_x_continuous(expand=(0, 0), limits=_x_lims)
        + gg.scale_y_continuous(expand=(0.02, 0, 0.02, 0), limits=_y_lims)
        + gg.labs(
            x=&#34;step&#34;,
            y=&#34;$\\log$ loss&#34; if y_log else &#34;loss&#34;,
            title=&#34;Approximation history&#34;,
        )
    )


def down_sample_ppc(
    ppc_ary: np.ndarray, n: int, axis: int = 0
) -&gt; tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;Down sample a PPC (or any numpy) array.

    Args:
        ppc_ary (np.ndarray): PPC array.
        n (int): Number of samples to return.
        axis (int, optional): The axis corresponding to the data samples. Defaults to 0.

    Returns:
        tuple[np.ndarray, np.ndarray]: A numpy array with `n` samples and an array of
        the indices sampled.
    &#34;&#34;&#34;
    r_idx = np.arange(ppc_ary.shape[axis])
    np.random.shuffle(r_idx)
    return ppc_ary[:, r_idx[:n]], r_idx</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="speclet.analysis.pymc3_analysis.down_sample_ppc"><code class="name flex">
<span>def <span class="ident">down_sample_ppc</span></span>(<span>ppc_ary: numpy.ndarray, n: int, axis: int = 0) -> tuple[numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Down sample a PPC (or any numpy) array.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ppc_ary</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>PPC array.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples to return.</dd>
<dt><strong><code>axis</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The axis corresponding to the data samples. Defaults to 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[np.ndarray, np.ndarray]</code></dt>
<dd>A numpy array with <code>n</code> samples and an array of</dd>
</dl>
<p>the indices sampled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def down_sample_ppc(
    ppc_ary: np.ndarray, n: int, axis: int = 0
) -&gt; tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;Down sample a PPC (or any numpy) array.

    Args:
        ppc_ary (np.ndarray): PPC array.
        n (int): Number of samples to return.
        axis (int, optional): The axis corresponding to the data samples. Defaults to 0.

    Returns:
        tuple[np.ndarray, np.ndarray]: A numpy array with `n` samples and an array of
        the indices sampled.
    &#34;&#34;&#34;
    r_idx = np.arange(ppc_ary.shape[axis])
    np.random.shuffle(r_idx)
    return ppc_ary[:, r_idx[:n]], r_idx</code></pre>
</details>
</dd>
<dt id="speclet.analysis.pymc3_analysis.plot_all_priors"><code class="name flex">
<span>def <span class="ident">plot_all_priors</span></span>(<span>prior_predictive: dict[str, numpy.ndarray], subplots: tuple[int, int], figsize: tuple[float, float], samples: int = 1000, rm_var_regex: str = 'log__|logodds_') -> tuple[matplotlib.figure.Figure, matplotlib.axes._axes.Axes]</span>
</code></dt>
<dd>
<div class="desc"><p>Plot all priors of a PyMC3 model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prior_predictive</code></strong> :&ensp;<code>dict[str, np.ndarray]</code></dt>
<dd>The results of sampling from the
priors of a PyMC3 model.</dd>
<dt><strong><code>subplots</code></strong> :&ensp;<code>tuple[int, int]</code></dt>
<dd>How many subplots to create.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple[float, float]</code></dt>
<dd>The size of the final figure.</dd>
<dt><strong><code>samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of samples from the distributions to use.
This can help the performance of the plotting if there are many samples.
Defaults to 1000.</dd>
<dt><strong><code>rm_var_regex</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>A regular expression for variables to remove.
Defaults to "log__|logodds_".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[Figure, Axes]</code></dt>
<dd>The matplotlib figure and array
of axes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_all_priors(
    prior_predictive: dict[str, np.ndarray],
    subplots: tuple[int, int],
    figsize: tuple[float, float],
    samples: int = 1000,
    rm_var_regex: str = &#34;log__|logodds_&#34;,
) -&gt; tuple[Figure, Axes]:
    &#34;&#34;&#34;Plot all priors of a PyMC3 model.

    Args:
        prior_predictive (dict[str, np.ndarray]): The results of sampling from the
          priors of a PyMC3 model.
        subplots (tuple[int, int]): How many subplots to create.
        figsize (tuple[float, float]): The size of the final figure.
        samples (int, optional): The number of samples from the distributions to use.
          This can help the performance of the plotting if there are many samples.
          Defaults to 1000.
        rm_var_regex (str, optional): A regular expression for variables to remove.
          Defaults to &#34;log__|logodds_&#34;.

    Returns:
        tuple[Figure, Axes]: The matplotlib figure and array
          of axes.
    &#34;&#34;&#34;
    model_vars: list[str] = []
    for x in prior_predictive:
        if not re.search(rm_var_regex, x):
            model_vars.append(x)

    model_vars.sort()
    model_vars.sort(key=lambda x: -len(x))

    fig, axes = plt.subplots(subplots[0], subplots[1], figsize=figsize)
    for var, ax in zip(model_vars, axes.flatten()):
        sns.kdeplot(x=np.random.choice(prior_predictive[var].flatten(), samples), ax=ax)
        ax.set_title(var)

    fig.tight_layout()
    return fig, axes</code></pre>
</details>
</dd>
<dt id="speclet.analysis.pymc3_analysis.plot_vi_hist"><code class="name flex">
<span>def <span class="ident">plot_vi_hist</span></span>(<span>approx: pymc3.variational.opvi.Approximation, y_log: bool = False, x_start: Union[int, float, NoneType] = None, rolling_window: int = 100) -> plotnine.ggplot.ggplot</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the history of fitting using Variational Inference.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>approx</code></strong> :&ensp;<code>pm.variational.Approximation</code></dt>
<dd>The approximation attribute from the
VI object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>gg.ggplot</code></dt>
<dd>A plot showing the fitting history.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_vi_hist(
    approx: pm.Approximation,
    y_log: bool = False,
    x_start: Optional[Union[int, float]] = None,
    rolling_window: int = 100,
) -&gt; gg.ggplot:
    &#34;&#34;&#34;Plot the history of fitting using Variational Inference.

    Args:
        approx (pm.variational.Approximation): The approximation attribute from the
          VI object.

    Returns:
        gg.ggplot: A plot showing the fitting history.
    &#34;&#34;&#34;
    y = &#34;np.log(loss)&#34; if y_log else &#34;loss&#34;
    rolling_y = &#34;np.log(loss_rolling_avg)&#34; if y_log else &#34;loss_rolling_avg&#34;
    d = (
        pd.DataFrame({&#34;loss&#34;: approx.hist})
        .assign(step=lambda d: np.arange(d.shape[0]))
        .pipe(_advi_hist_rolling_avg, window=rolling_window)
    )

    _x_lims, _y_lims = _plot_vi_axes_limits(approx.hist, x_start)
    if y_log:
        _y_lims = np.log(_y_lims).tolist()

    return (
        gg.ggplot(d, gg.aes(x=&#34;step&#34;))
        + gg.geom_line(gg.aes(y=y), size=0.5, alpha=0.75, color=&#34;black&#34;)
        + gg.geom_line(gg.aes(y=rolling_y), size=0.5, alpha=0.9, color=SeabornColor.RED)
        + gg.scale_x_continuous(expand=(0, 0), limits=_x_lims)
        + gg.scale_y_continuous(expand=(0.02, 0, 0.02, 0), limits=_y_lims)
        + gg.labs(
            x=&#34;step&#34;,
            y=&#34;$\\log$ loss&#34; if y_log else &#34;loss&#34;,
            title=&#34;Approximation history&#34;,
        )
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="speclet.analysis" href="index.html">speclet.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="speclet.analysis.pymc3_analysis.down_sample_ppc" href="#speclet.analysis.pymc3_analysis.down_sample_ppc">down_sample_ppc</a></code></li>
<li><code><a title="speclet.analysis.pymc3_analysis.plot_all_priors" href="#speclet.analysis.pymc3_analysis.plot_all_priors">plot_all_priors</a></code></li>
<li><code><a title="speclet.analysis.pymc3_analysis.plot_vi_hist" href="#speclet.analysis.pymc3_analysis.plot_vi_hist">plot_vi_hist</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>