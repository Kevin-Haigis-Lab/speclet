<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>src.analysis.pymc3_analysis API documentation</title>
<meta name="description" content="Functions to aid in the analysis of PyMC3 models." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.analysis.pymc3_analysis</code></h1>
</header>
<section id="section-intro">
<p>Functions to aid in the analysis of PyMC3 models.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Functions to aid in the analysis of PyMC3 models.&#34;&#34;&#34;

import datetime
import re
from typing import Optional, Sequence, Union

import arviz as az
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotnine as gg
import pymc3 as pm
import seaborn as sns
from pydantic import BaseModel
from xarray import Dataset

from src.plot.color_pal import SeabornColor


def plot_all_priors(
    prior_predictive: dict[str, np.ndarray],
    subplots: tuple[int, int],
    figsize: tuple[float, float],
    samples: int = 1000,
    rm_var_regex: str = &#34;log__|logodds_&#34;,
) -&gt; tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]:
    &#34;&#34;&#34;Plot all priors of a PyMC3 model.

    Args:
        prior_predictive (dict[str, np.ndarray]): The results of sampling from the
          priors of a PyMC3 model.
        subplots (tuple[int, int]): How many subplots to create.
        figsize (tuple[float, float]): The size of the final figure.
        samples (int, optional): The number of samples from the distributions to use.
          This can help the performance of the plotting if there are many samples.
          Defaults to 1000.
        rm_var_regex (str, optional): A regular expression for variables to remove.
          Defaults to &#34;log__|logodds_&#34;.

    Returns:
        tuple[matplotlib.figure.Figure, np.ndarray]: The matplotlib figure and array
          of axes.
    &#34;&#34;&#34;
    model_vars: list[str] = []
    for x in prior_predictive:
        if not re.search(rm_var_regex, x):
            model_vars.append(x)

    model_vars.sort()
    model_vars.sort(key=lambda x: -len(x))

    fig, axes = plt.subplots(subplots[0], subplots[1], figsize=figsize)
    for var, ax in zip(model_vars, axes.flatten()):
        sns.kdeplot(x=np.random.choice(prior_predictive[var].flatten(), samples), ax=ax)
        ax.set_title(var)

    fig.tight_layout()
    return fig, axes


def extract_matrix_variable_indices(
    d: pd.DataFrame,
    col: str,
    idx1: np.ndarray,
    idx2: np.ndarray,
    idx1name: str,
    idx2name: str,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Extract and annotating matrix (2D only) indices.

    Args:
        d (pd.DataFrame): The data frame produced by summarizing the posteriors of a
          PyMC3 model.
        col (str): The column with the 2D indices.
        idx1 (np.ndarray): The values to use for the first index.
        idx2 (np.ndarray): The values to use for the second index.
        idx1name (str): The name to give the first index.
        idx2name (str): The name to give the second index.

    Returns:
        pd.DataFrame: The modified data frame.
    &#34;&#34;&#34;
    indices_list = [
        [int(x) for x in re.findall(&#34;[0-9]+&#34;, s)] for s in d[[col]].to_numpy().flatten()
    ]
    indices_array = np.asarray(indices_list)
    d[idx1name] = idx1[indices_array[:, 0]]
    d[idx2name] = idx2[indices_array[:, 1]]
    return d


def _reshape_mcmc_chains_to_2d(a: np.ndarray) -&gt; np.ndarray:
    z = a.shape[2]
    return a.reshape(1, -1, z).squeeze()


def summarize_posterior_predictions(
    a: np.ndarray,
    hdi_prob: float = 0.89,
    merge_with: Optional[pd.DataFrame] = None,
    calc_error: bool = False,
    observed_y: Optional[str] = None,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Summarizing PyMC3 PPCs.

    Args:
        a (np.ndarray): The posterior predictions.
        hdi_prob (float, optional): The HDI probability to use. Defaults to 0.89.
        merge_with (Optional[pd.DataFrame], optional): The original data to merge with
          the predictions. Defaults to None.
        calc_error (bool): Should the error (real - predicted) be calculated? This is
          only used if `merge_with` is not None. Default to false.
        observed_y: (Optional[str], optional): The column with the observed data. This
          is only used if `merge_with` is not None and `calc_error` is true. Default
          to None.

    Returns:
        pd.DataFrame: A data frame with one row per data point and columns describing
          the posterior predictions.
    &#34;&#34;&#34;
    if len(a.shape) == 3:
        a = _reshape_mcmc_chains_to_2d(a)
    hdi = az.hdi(a, hdi_prob=hdi_prob)

    d = pd.DataFrame(
        {
            &#34;pred_mean&#34;: a.mean(axis=0),
            &#34;pred_hdi_low&#34;: hdi[:, 0],
            &#34;pred_hdi_high&#34;: hdi[:, 1],
        }
    )

    if merge_with is not None:
        d = pd.merge(
            d, merge_with.reset_index(drop=True), left_index=True, right_index=True
        )
        if calc_error and observed_y is not None:
            if observed_y not in d.columns:
                raise TypeError(f&#34;Column &#39;{observed_y}&#39; is not in data.&#34;)
            d[&#34;error&#34;] = d[observed_y].values - d[&#34;pred_mean&#34;].values

    return d


def _plot_vi_axes_limits(
    yvals: np.ndarray, x_start: Optional[Union[int, float]]
) -&gt; tuple[list[int], list[float]]:
    yvals = yvals.copy()[np.isfinite(yvals)]
    x_lims: list[int] = [0, len(yvals)]
    if x_start is None:
        pass
    elif isinstance(x_start, float) and x_start &lt;= 1.0:
        x_lims[0] = int(x_start * len(yvals))
    else:
        x_lims[0] = int(x_start)

    y_lims: list[float] = [min(yvals), max(yvals[x_lims[0] :])]

    return x_lims, y_lims


def _advi_hist_rolling_avg(df: pd.DataFrame, window: int) -&gt; pd.DataFrame:
    df[&#34;loss_rolling_avg&#34;] = (
        df[[&#34;loss&#34;]].rolling(window=window, center=True).mean()[&#34;loss&#34;]
    )
    return df


def plot_vi_hist(
    approx: pm.Approximation,
    y_log: bool = False,
    x_start: Optional[Union[int, float]] = None,
    rolling_window: int = 100,
) -&gt; gg.ggplot:
    &#34;&#34;&#34;Plot the history of fitting using Variational Inference.

    Args:
        approx (pm.variational.Approximation): The approximation attribute from the
          VI object.

    Returns:
        gg.ggplot: A plot showing the fitting history.
    &#34;&#34;&#34;
    y = &#34;np.log(loss)&#34; if y_log else &#34;loss&#34;
    rolling_y = &#34;np.log(loss_rolling_avg)&#34; if y_log else &#34;loss_rolling_avg&#34;
    d = (
        pd.DataFrame({&#34;loss&#34;: approx.hist})
        .assign(step=lambda d: np.arange(d.shape[0]))
        .pipe(_advi_hist_rolling_avg, window=rolling_window)
    )

    _x_lims, _y_lims = _plot_vi_axes_limits(approx.hist, x_start)
    if y_log:
        _y_lims = np.log(_y_lims)

    return (
        gg.ggplot(d, gg.aes(x=&#34;step&#34;))
        + gg.geom_line(gg.aes(y=y), size=0.5, alpha=0.75, color=&#34;black&#34;)
        + gg.geom_line(gg.aes(y=rolling_y), size=0.5, alpha=0.9, color=SeabornColor.RED)
        + gg.scale_x_continuous(expand=(0, 0), limits=_x_lims)
        + gg.scale_y_continuous(expand=(0.02, 0, 0.02, 0), limits=_y_lims)
        + gg.labs(
            x=&#34;step&#34;,
            y=&#34;$\\log$ loss&#34; if y_log else &#34;loss&#34;,
            title=&#34;Approximation history&#34;,
        )
    )


def get_hdi_colnames_from_az_summary(df: pd.DataFrame) -&gt; tuple[str, str]:
    &#34;&#34;&#34;Get the column names corresponding to the HDI from an ArviZ summary.

    Args:
        df (pd.DataFrame): ArviZ posterior summary data frame.

    Returns:
        tuple[str, str]: The two column names.
    &#34;&#34;&#34;
    cols: list[str] = [c for c in df.columns if &#34;hdi_&#34; in c]
    cols = [c for c in cols if &#34;%&#34; in c]
    assert len(cols) == 2
    return cols[0], cols[1]


def _pretty_bfmi(data: az.InferenceData, decimals: int = 3) -&gt; list[str]:
    return np.round(az.bfmi(data), decimals).astype(str).tolist()


def get_average_step_size(data: az.InferenceData) -&gt; list[float]:
    &#34;&#34;&#34;Get the average step size for each chain of MCMC.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        list[float]: list of average step sizes for each chain.
    &#34;&#34;&#34;
    return data.sample_stats.step_size.mean(axis=1).values.tolist()


def _pretty_step_size(data: az.InferenceData, decimals: int = 3) -&gt; list[str]:
    return np.round(get_average_step_size(data), decimals).astype(str).tolist()


def get_divergences(data: az.InferenceData) -&gt; np.ndarray:
    &#34;&#34;&#34;Get the number and percent of steps that were divergences of each MCMC chain.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        tuple[list[int], list[float]]: A list of the number of divergent steps and a
        list of the percent of steps that were divergent.
    &#34;&#34;&#34;
    return data.sample_stats.diverging.values


def get_divergence_summary(data: az.InferenceData) -&gt; tuple[list[int], list[float]]:
    &#34;&#34;&#34;Get the number and percent of steps that were divergences of each MCMC chain.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        tuple[list[int], list[float]]: A list of the number of divergent steps and a
        list of the percent of steps that were divergent.
    &#34;&#34;&#34;
    divs = data.sample_stats.diverging.values
    totals = divs.sum(axis=1)
    pct = divs.mean(axis=1) * 100
    return totals.tolist(), pct.tolist()


class MCMCDescription(BaseModel):
    &#34;&#34;&#34;Descriptive information for a MCMC.&#34;&#34;&#34;

    created: Optional[datetime.datetime]
    duration: Optional[datetime.timedelta]
    n_chains: int
    n_tuning_steps: Optional[int]
    n_draws: int
    n_divergences: list[int]
    pct_divergences: list[float]
    bfmi: list[float]
    avg_step_size: list[float]

    def _pretty_list(self, vals: Sequence[Union[int, float]], round: int = 3) -&gt; str:
        return &#34;, &#34;.join(np.round(vals, round).astype(str).tolist())

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Nifty ol&#39; string.&#34;&#34;&#34;
        messages: list[str] = []
        if self.created is not None:
            messages.append(f&#34;date created: {self.created:%Y-%m-%d %H:%M}&#34;)
        if self.duration is not None:
            _d_min = self.duration / datetime.timedelta(minutes=1)
            messages.append(f&#34;time required: {_d_min:0.2f} minutes&#34;)
        _n_tuning_steps = (
            f&#34;{self.n_tuning_steps:,}&#34;
            if (self.n_tuning_steps is not None)
            else &#34;(unknown)&#34;
        )
        messages.append(
            f&#34;sampled {self.n_chains} chains with {_n_tuning_steps} &#34;
            + f&#34;tuning steps and {self.n_draws:,} draws&#34;
        )
        messages.append(f&#34;num. divergences: {self._pretty_list(self.n_divergences)}&#34;)
        messages.append(
            f&#34;percent divergences: {self._pretty_list(self.pct_divergences)}&#34;
        )
        messages.append(f&#34;BFMI: {self._pretty_list(self.bfmi)}&#34;)
        messages.append(f&#34;avg. step size: {self._pretty_list(self.avg_step_size)}&#34;)
        return &#34;\n&#34;.join(messages)


def describe_mcmc(
    data: az.InferenceData, silent: bool = False, plot: bool = True
) -&gt; MCMCDescription:
    &#34;&#34;&#34;Descriptive statistics and plots for MCMC.

    Prints out the following:

    1. Date of creation and how long the sampling took. ***
    2. The number of tuning and sampling steps. ***
    3. BFMI of each chain.
    4. Average step size of each chain.
    5. Number of divergences in each chain.
    6. Plot the energy transition distribution and marginal energy distribution.

    Args:
        data (az.InferenceData): Data object.
        silent (bool, optional): Silence the printing of the description? Defaults to
          False.
        plot (bool, optional): Include any plots? Default is True.
    &#34;&#34;&#34;
    if not hasattr(data, &#34;sample_stats&#34;):
        print(&#34;Unable to get sampling stats.&#34;)
        raise AttributeError(&#34;Input data does not have a `sample_stats` attribute.&#34;)

    sample_stats = data.get(&#34;sample_stats&#34;)
    if not isinstance(sample_stats, Dataset):
        raise AttributeError(&#34;`sample_stats` attribute is not of type `xarray.Dataset`&#34;)

    # Date and duration.
    created_at = sample_stats.get(&#34;created_at&#34;)
    duration: Optional[datetime.timedelta] = None
    if (duration_sec := sample_stats.get(&#34;sampling_time&#34;)) is not None:
        duration = datetime.timedelta(seconds=duration_sec)

    # Sampling dimensions
    n_tuning_steps: Optional[int] = sample_stats.get(&#34;tuning_steps&#34;)
    n_draws: int = len(sample_stats.draw)
    n_chains: int = len(sample_stats.chain)

    # Divergences
    n_divergences, pct_divergences = get_divergence_summary(data)

    # BFMI.
    bfmi = az.bfmi(data).tolist()

    # Average step size.
    avg_step_size = get_average_step_size(data)

    mcmc_descr = MCMCDescription(
        created=created_at,
        duration=duration,
        n_tuning_steps=n_tuning_steps,
        n_chains=n_chains,
        n_draws=n_draws,
        n_divergences=n_divergences,
        pct_divergences=pct_divergences,
        bfmi=bfmi,
        avg_step_size=avg_step_size,
    )

    if not silent:
        print(mcmc_descr)

    if plot:
        az.plot_energy(data)
        plt.show()

    return mcmc_descr</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.analysis.pymc3_analysis.describe_mcmc"><code class="name flex">
<span>def <span class="ident">describe_mcmc</span></span>(<span>data: arviz.data.inference_data.InferenceData, silent: bool = False, plot: bool = True) -> <a title="src.analysis.pymc3_analysis.MCMCDescription" href="#src.analysis.pymc3_analysis.MCMCDescription">MCMCDescription</a></span>
</code></dt>
<dd>
<div class="desc"><p>Descriptive statistics and plots for MCMC.</p>
<p>Prints out the following:</p>
<ol>
<li>Date of creation and how long the sampling took. ***</li>
<li>The number of tuning and sampling steps. ***</li>
<li>BFMI of each chain.</li>
<li>Average step size of each chain.</li>
<li>Number of divergences in each chain.</li>
<li>Plot the energy transition distribution and marginal energy distribution.</li>
</ol>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>az.InferenceData</code></dt>
<dd>Data object.</dd>
<dt><strong><code>silent</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Silence the printing of the description? Defaults to
False.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Include any plots? Default is True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def describe_mcmc(
    data: az.InferenceData, silent: bool = False, plot: bool = True
) -&gt; MCMCDescription:
    &#34;&#34;&#34;Descriptive statistics and plots for MCMC.

    Prints out the following:

    1. Date of creation and how long the sampling took. ***
    2. The number of tuning and sampling steps. ***
    3. BFMI of each chain.
    4. Average step size of each chain.
    5. Number of divergences in each chain.
    6. Plot the energy transition distribution and marginal energy distribution.

    Args:
        data (az.InferenceData): Data object.
        silent (bool, optional): Silence the printing of the description? Defaults to
          False.
        plot (bool, optional): Include any plots? Default is True.
    &#34;&#34;&#34;
    if not hasattr(data, &#34;sample_stats&#34;):
        print(&#34;Unable to get sampling stats.&#34;)
        raise AttributeError(&#34;Input data does not have a `sample_stats` attribute.&#34;)

    sample_stats = data.get(&#34;sample_stats&#34;)
    if not isinstance(sample_stats, Dataset):
        raise AttributeError(&#34;`sample_stats` attribute is not of type `xarray.Dataset`&#34;)

    # Date and duration.
    created_at = sample_stats.get(&#34;created_at&#34;)
    duration: Optional[datetime.timedelta] = None
    if (duration_sec := sample_stats.get(&#34;sampling_time&#34;)) is not None:
        duration = datetime.timedelta(seconds=duration_sec)

    # Sampling dimensions
    n_tuning_steps: Optional[int] = sample_stats.get(&#34;tuning_steps&#34;)
    n_draws: int = len(sample_stats.draw)
    n_chains: int = len(sample_stats.chain)

    # Divergences
    n_divergences, pct_divergences = get_divergence_summary(data)

    # BFMI.
    bfmi = az.bfmi(data).tolist()

    # Average step size.
    avg_step_size = get_average_step_size(data)

    mcmc_descr = MCMCDescription(
        created=created_at,
        duration=duration,
        n_tuning_steps=n_tuning_steps,
        n_chains=n_chains,
        n_draws=n_draws,
        n_divergences=n_divergences,
        pct_divergences=pct_divergences,
        bfmi=bfmi,
        avg_step_size=avg_step_size,
    )

    if not silent:
        print(mcmc_descr)

    if plot:
        az.plot_energy(data)
        plt.show()

    return mcmc_descr</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.extract_matrix_variable_indices"><code class="name flex">
<span>def <span class="ident">extract_matrix_variable_indices</span></span>(<span>d: pandas.core.frame.DataFrame, col: str, idx1: numpy.ndarray, idx2: numpy.ndarray, idx1name: str, idx2name: str) -> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Extract and annotating matrix (2D only) indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>d</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The data frame produced by summarizing the posteriors of a
PyMC3 model.</dd>
<dt><strong><code>col</code></strong> :&ensp;<code>str</code></dt>
<dd>The column with the 2D indices.</dd>
<dt><strong><code>idx1</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The values to use for the first index.</dd>
<dt><strong><code>idx2</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The values to use for the second index.</dd>
<dt><strong><code>idx1name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name to give the first index.</dd>
<dt><strong><code>idx2name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name to give the second index.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>The modified data frame.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_matrix_variable_indices(
    d: pd.DataFrame,
    col: str,
    idx1: np.ndarray,
    idx2: np.ndarray,
    idx1name: str,
    idx2name: str,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Extract and annotating matrix (2D only) indices.

    Args:
        d (pd.DataFrame): The data frame produced by summarizing the posteriors of a
          PyMC3 model.
        col (str): The column with the 2D indices.
        idx1 (np.ndarray): The values to use for the first index.
        idx2 (np.ndarray): The values to use for the second index.
        idx1name (str): The name to give the first index.
        idx2name (str): The name to give the second index.

    Returns:
        pd.DataFrame: The modified data frame.
    &#34;&#34;&#34;
    indices_list = [
        [int(x) for x in re.findall(&#34;[0-9]+&#34;, s)] for s in d[[col]].to_numpy().flatten()
    ]
    indices_array = np.asarray(indices_list)
    d[idx1name] = idx1[indices_array[:, 0]]
    d[idx2name] = idx2[indices_array[:, 1]]
    return d</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.get_average_step_size"><code class="name flex">
<span>def <span class="ident">get_average_step_size</span></span>(<span>data: arviz.data.inference_data.InferenceData) -> list</span>
</code></dt>
<dd>
<div class="desc"><p>Get the average step size for each chain of MCMC.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>az.InferenceData</code></dt>
<dd>Data object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[float]</code></dt>
<dd>list of average step sizes for each chain.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_average_step_size(data: az.InferenceData) -&gt; list[float]:
    &#34;&#34;&#34;Get the average step size for each chain of MCMC.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        list[float]: list of average step sizes for each chain.
    &#34;&#34;&#34;
    return data.sample_stats.step_size.mean(axis=1).values.tolist()</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.get_divergence_summary"><code class="name flex">
<span>def <span class="ident">get_divergence_summary</span></span>(<span>data: arviz.data.inference_data.InferenceData) -> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Get the number and percent of steps that were divergences of each MCMC chain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>az.InferenceData</code></dt>
<dd>Data object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[list[int], list[float]]</code></dt>
<dd>A list of the number of divergent steps and a</dd>
</dl>
<p>list of the percent of steps that were divergent.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_divergence_summary(data: az.InferenceData) -&gt; tuple[list[int], list[float]]:
    &#34;&#34;&#34;Get the number and percent of steps that were divergences of each MCMC chain.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        tuple[list[int], list[float]]: A list of the number of divergent steps and a
        list of the percent of steps that were divergent.
    &#34;&#34;&#34;
    divs = data.sample_stats.diverging.values
    totals = divs.sum(axis=1)
    pct = divs.mean(axis=1) * 100
    return totals.tolist(), pct.tolist()</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.get_divergences"><code class="name flex">
<span>def <span class="ident">get_divergences</span></span>(<span>data: arviz.data.inference_data.InferenceData) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Get the number and percent of steps that were divergences of each MCMC chain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>az.InferenceData</code></dt>
<dd>Data object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[list[int], list[float]]</code></dt>
<dd>A list of the number of divergent steps and a</dd>
</dl>
<p>list of the percent of steps that were divergent.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_divergences(data: az.InferenceData) -&gt; np.ndarray:
    &#34;&#34;&#34;Get the number and percent of steps that were divergences of each MCMC chain.

    Args:
        data (az.InferenceData): Data object.

    Returns:
        tuple[list[int], list[float]]: A list of the number of divergent steps and a
        list of the percent of steps that were divergent.
    &#34;&#34;&#34;
    return data.sample_stats.diverging.values</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.get_hdi_colnames_from_az_summary"><code class="name flex">
<span>def <span class="ident">get_hdi_colnames_from_az_summary</span></span>(<span>df: pandas.core.frame.DataFrame) -> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Get the column names corresponding to the HDI from an ArviZ summary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>ArviZ posterior summary data frame.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str, str]</code></dt>
<dd>The two column names.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hdi_colnames_from_az_summary(df: pd.DataFrame) -&gt; tuple[str, str]:
    &#34;&#34;&#34;Get the column names corresponding to the HDI from an ArviZ summary.

    Args:
        df (pd.DataFrame): ArviZ posterior summary data frame.

    Returns:
        tuple[str, str]: The two column names.
    &#34;&#34;&#34;
    cols: list[str] = [c for c in df.columns if &#34;hdi_&#34; in c]
    cols = [c for c in cols if &#34;%&#34; in c]
    assert len(cols) == 2
    return cols[0], cols[1]</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.plot_all_priors"><code class="name flex">
<span>def <span class="ident">plot_all_priors</span></span>(<span>prior_predictive: dict, subplots: tuple, figsize: tuple, samples: int = 1000, rm_var_regex: str = 'log__|logodds_') -> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Plot all priors of a PyMC3 model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prior_predictive</code></strong> :&ensp;<code>dict[str, np.ndarray]</code></dt>
<dd>The results of sampling from the
priors of a PyMC3 model.</dd>
<dt><strong><code>subplots</code></strong> :&ensp;<code>tuple[int, int]</code></dt>
<dd>How many subplots to create.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple[float, float]</code></dt>
<dd>The size of the final figure.</dd>
<dt><strong><code>samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of samples from the distributions to use.
This can help the performance of the plotting if there are many samples.
Defaults to 1000.</dd>
<dt><strong><code>rm_var_regex</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>A regular expression for variables to remove.
Defaults to "log__|logodds_".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[matplotlib.figure.Figure, np.ndarray]</code></dt>
<dd>The matplotlib figure and array
of axes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_all_priors(
    prior_predictive: dict[str, np.ndarray],
    subplots: tuple[int, int],
    figsize: tuple[float, float],
    samples: int = 1000,
    rm_var_regex: str = &#34;log__|logodds_&#34;,
) -&gt; tuple[matplotlib.figure.Figure, matplotlib.axes.Axes]:
    &#34;&#34;&#34;Plot all priors of a PyMC3 model.

    Args:
        prior_predictive (dict[str, np.ndarray]): The results of sampling from the
          priors of a PyMC3 model.
        subplots (tuple[int, int]): How many subplots to create.
        figsize (tuple[float, float]): The size of the final figure.
        samples (int, optional): The number of samples from the distributions to use.
          This can help the performance of the plotting if there are many samples.
          Defaults to 1000.
        rm_var_regex (str, optional): A regular expression for variables to remove.
          Defaults to &#34;log__|logodds_&#34;.

    Returns:
        tuple[matplotlib.figure.Figure, np.ndarray]: The matplotlib figure and array
          of axes.
    &#34;&#34;&#34;
    model_vars: list[str] = []
    for x in prior_predictive:
        if not re.search(rm_var_regex, x):
            model_vars.append(x)

    model_vars.sort()
    model_vars.sort(key=lambda x: -len(x))

    fig, axes = plt.subplots(subplots[0], subplots[1], figsize=figsize)
    for var, ax in zip(model_vars, axes.flatten()):
        sns.kdeplot(x=np.random.choice(prior_predictive[var].flatten(), samples), ax=ax)
        ax.set_title(var)

    fig.tight_layout()
    return fig, axes</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.plot_vi_hist"><code class="name flex">
<span>def <span class="ident">plot_vi_hist</span></span>(<span>approx: pymc3.variational.opvi.Approximation, y_log: bool = False, x_start: Union[int, float, NoneType] = None, rolling_window: int = 100) -> plotnine.ggplot.ggplot</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the history of fitting using Variational Inference.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>approx</code></strong> :&ensp;<code>pm.variational.Approximation</code></dt>
<dd>The approximation attribute from the
VI object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>gg.ggplot</code></dt>
<dd>A plot showing the fitting history.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_vi_hist(
    approx: pm.Approximation,
    y_log: bool = False,
    x_start: Optional[Union[int, float]] = None,
    rolling_window: int = 100,
) -&gt; gg.ggplot:
    &#34;&#34;&#34;Plot the history of fitting using Variational Inference.

    Args:
        approx (pm.variational.Approximation): The approximation attribute from the
          VI object.

    Returns:
        gg.ggplot: A plot showing the fitting history.
    &#34;&#34;&#34;
    y = &#34;np.log(loss)&#34; if y_log else &#34;loss&#34;
    rolling_y = &#34;np.log(loss_rolling_avg)&#34; if y_log else &#34;loss_rolling_avg&#34;
    d = (
        pd.DataFrame({&#34;loss&#34;: approx.hist})
        .assign(step=lambda d: np.arange(d.shape[0]))
        .pipe(_advi_hist_rolling_avg, window=rolling_window)
    )

    _x_lims, _y_lims = _plot_vi_axes_limits(approx.hist, x_start)
    if y_log:
        _y_lims = np.log(_y_lims)

    return (
        gg.ggplot(d, gg.aes(x=&#34;step&#34;))
        + gg.geom_line(gg.aes(y=y), size=0.5, alpha=0.75, color=&#34;black&#34;)
        + gg.geom_line(gg.aes(y=rolling_y), size=0.5, alpha=0.9, color=SeabornColor.RED)
        + gg.scale_x_continuous(expand=(0, 0), limits=_x_lims)
        + gg.scale_y_continuous(expand=(0.02, 0, 0.02, 0), limits=_y_lims)
        + gg.labs(
            x=&#34;step&#34;,
            y=&#34;$\\log$ loss&#34; if y_log else &#34;loss&#34;,
            title=&#34;Approximation history&#34;,
        )
    )</code></pre>
</details>
</dd>
<dt id="src.analysis.pymc3_analysis.summarize_posterior_predictions"><code class="name flex">
<span>def <span class="ident">summarize_posterior_predictions</span></span>(<span>a: numpy.ndarray, hdi_prob: float = 0.89, merge_with: Optional[pandas.core.frame.DataFrame] = None, calc_error: bool = False, observed_y: Optional[str] = None) -> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Summarizing PyMC3 PPCs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The posterior predictions.</dd>
<dt><strong><code>hdi_prob</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The HDI probability to use. Defaults to 0.89.</dd>
<dt><strong><code>merge_with</code></strong> :&ensp;<code>Optional[pd.DataFrame]</code>, optional</dt>
<dd>The original data to merge with
the predictions. Defaults to None.</dd>
<dt><strong><code>calc_error</code></strong> :&ensp;<code>bool</code></dt>
<dd>Should the error (real - predicted) be calculated? This is
only used if <code>merge_with</code> is not None. Default to false.</dd>
<dt><strong><code>observed_y</code></strong></dt>
<dd>(Optional[str], optional): The column with the observed data. This
is only used if <code>merge_with</code> is not None and <code>calc_error</code> is true. Default
to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A data frame with one row per data point and columns describing
the posterior predictions.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def summarize_posterior_predictions(
    a: np.ndarray,
    hdi_prob: float = 0.89,
    merge_with: Optional[pd.DataFrame] = None,
    calc_error: bool = False,
    observed_y: Optional[str] = None,
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Summarizing PyMC3 PPCs.

    Args:
        a (np.ndarray): The posterior predictions.
        hdi_prob (float, optional): The HDI probability to use. Defaults to 0.89.
        merge_with (Optional[pd.DataFrame], optional): The original data to merge with
          the predictions. Defaults to None.
        calc_error (bool): Should the error (real - predicted) be calculated? This is
          only used if `merge_with` is not None. Default to false.
        observed_y: (Optional[str], optional): The column with the observed data. This
          is only used if `merge_with` is not None and `calc_error` is true. Default
          to None.

    Returns:
        pd.DataFrame: A data frame with one row per data point and columns describing
          the posterior predictions.
    &#34;&#34;&#34;
    if len(a.shape) == 3:
        a = _reshape_mcmc_chains_to_2d(a)
    hdi = az.hdi(a, hdi_prob=hdi_prob)

    d = pd.DataFrame(
        {
            &#34;pred_mean&#34;: a.mean(axis=0),
            &#34;pred_hdi_low&#34;: hdi[:, 0],
            &#34;pred_hdi_high&#34;: hdi[:, 1],
        }
    )

    if merge_with is not None:
        d = pd.merge(
            d, merge_with.reset_index(drop=True), left_index=True, right_index=True
        )
        if calc_error and observed_y is not None:
            if observed_y not in d.columns:
                raise TypeError(f&#34;Column &#39;{observed_y}&#39; is not in data.&#34;)
            d[&#34;error&#34;] = d[observed_y].values - d[&#34;pred_mean&#34;].values

    return d</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.analysis.pymc3_analysis.MCMCDescription"><code class="flex name class">
<span>class <span class="ident">MCMCDescription</span></span>
<span>(</span><span>*, created: datetime.datetime = None, duration: datetime.timedelta = None, n_chains: int, n_tuning_steps: int = None, n_draws: int, n_divergences: list, pct_divergences: list, bfmi: list, avg_step_size: list)</span>
</code></dt>
<dd>
<div class="desc"><p>Descriptive information for a MCMC.</p>
<p>Create a new model by parsing and validating input data from keyword arguments.</p>
<p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MCMCDescription(BaseModel):
    &#34;&#34;&#34;Descriptive information for a MCMC.&#34;&#34;&#34;

    created: Optional[datetime.datetime]
    duration: Optional[datetime.timedelta]
    n_chains: int
    n_tuning_steps: Optional[int]
    n_draws: int
    n_divergences: list[int]
    pct_divergences: list[float]
    bfmi: list[float]
    avg_step_size: list[float]

    def _pretty_list(self, vals: Sequence[Union[int, float]], round: int = 3) -&gt; str:
        return &#34;, &#34;.join(np.round(vals, round).astype(str).tolist())

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Nifty ol&#39; string.&#34;&#34;&#34;
        messages: list[str] = []
        if self.created is not None:
            messages.append(f&#34;date created: {self.created:%Y-%m-%d %H:%M}&#34;)
        if self.duration is not None:
            _d_min = self.duration / datetime.timedelta(minutes=1)
            messages.append(f&#34;time required: {_d_min:0.2f} minutes&#34;)
        _n_tuning_steps = (
            f&#34;{self.n_tuning_steps:,}&#34;
            if (self.n_tuning_steps is not None)
            else &#34;(unknown)&#34;
        )
        messages.append(
            f&#34;sampled {self.n_chains} chains with {_n_tuning_steps} &#34;
            + f&#34;tuning steps and {self.n_draws:,} draws&#34;
        )
        messages.append(f&#34;num. divergences: {self._pretty_list(self.n_divergences)}&#34;)
        messages.append(
            f&#34;percent divergences: {self._pretty_list(self.pct_divergences)}&#34;
        )
        messages.append(f&#34;BFMI: {self._pretty_list(self.bfmi)}&#34;)
        messages.append(f&#34;avg. step size: {self._pretty_list(self.avg_step_size)}&#34;)
        return &#34;\n&#34;.join(messages)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pydantic.main.BaseModel</li>
<li>pydantic.utils.Representation</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.analysis" href="index.html">src.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.analysis.pymc3_analysis.describe_mcmc" href="#src.analysis.pymc3_analysis.describe_mcmc">describe_mcmc</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.extract_matrix_variable_indices" href="#src.analysis.pymc3_analysis.extract_matrix_variable_indices">extract_matrix_variable_indices</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.get_average_step_size" href="#src.analysis.pymc3_analysis.get_average_step_size">get_average_step_size</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.get_divergence_summary" href="#src.analysis.pymc3_analysis.get_divergence_summary">get_divergence_summary</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.get_divergences" href="#src.analysis.pymc3_analysis.get_divergences">get_divergences</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.get_hdi_colnames_from_az_summary" href="#src.analysis.pymc3_analysis.get_hdi_colnames_from_az_summary">get_hdi_colnames_from_az_summary</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.plot_all_priors" href="#src.analysis.pymc3_analysis.plot_all_priors">plot_all_priors</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.plot_vi_hist" href="#src.analysis.pymc3_analysis.plot_vi_hist">plot_vi_hist</a></code></li>
<li><code><a title="src.analysis.pymc3_analysis.summarize_posterior_predictions" href="#src.analysis.pymc3_analysis.summarize_posterior_predictions">summarize_posterior_predictions</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.analysis.pymc3_analysis.MCMCDescription" href="#src.analysis.pymc3_analysis.MCMCDescription">MCMCDescription</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>